{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d3f75a2-ae58-4cb2-afe5-741c6b0ea3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import wrds\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats.mstats import winsorize\n",
    "from scipy.stats import ttest_1samp\n",
    "from scipy.stats import kurtosis, skew\n",
    "import pandas as pd\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.stats import mstats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2094453-aeda-49f8-9f22-b8a2fd5714ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading library list...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "db = wrds.Connection(wrds_username='charly99')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77106025-b926-43a1-a955-154f68ddd2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input directory\n",
    "input_directory = '/work/pi_atreya_chakraborty_umb_edu/Captsone/Data'\n",
    "\n",
    "# Define the input file paths\n",
    "clean_analyst_path_weekly = os.path.join(input_directory, 'clean_analyst_weekly.dta')\n",
    "clean_analyst_path_monthly = os.path.join(input_directory, 'clean_analyst_monthly.dta')\n",
    "clean_crsp_path = os.path.join(input_directory, 'clean_crsp.dta')\n",
    "\n",
    "# Read the datasets\n",
    "results_df_weekly = pd.read_stata(clean_analyst_path_weekly)\n",
    "results_df_monthly = pd.read_stata(clean_analyst_path_monthly)\n",
    "crsp_data = pd.read_stata(clean_crsp_path)\n",
    "\n",
    "results_df_weekly = results_df_weekly.groupby(['cusip', 'estimid', 'year', 'month', 'week']).first().reset_index()\n",
    "results_df_monthly = results_df_monthly.groupby(['cusip', 'estimid', 'year', 'month']).first().reset_index()\n",
    "\n",
    "# Convert 'ireccd' to numeric, forcing errors to NaN (if any)\n",
    "results_df_monthly['ireccd'] = pd.to_numeric(results_df_monthly['ireccd'], errors='coerce')\n",
    "\n",
    "# Convert 'ireccd' to numeric, forcing errors to NaN (if any)\n",
    "results_df_weekly['ireccd'] = pd.to_numeric(results_df_monthly['ireccd'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57439017-482a-45a6-8d1d-552655bd3068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the transformation (6 - ireccd)\n",
    "results_df_monthly['ireccd_transformed'] = 6 - results_df_monthly['ireccd']\n",
    "\n",
    "# Define a function to calculate descriptive statistics, including Q1, Q3, kurtosis, and skewness\n",
    "def get_descriptive_stats(group):\n",
    "    return pd.Series({\n",
    "        'mean': group.mean(),\n",
    "        'median': group.median(),\n",
    "        'std': group.std(),\n",
    "        'min': group.min(),\n",
    "        'max': group.max(),\n",
    "        'count': group.count(),\n",
    "        'skewness': skew(group, nan_policy='omit'),\n",
    "        'kurtosis': kurtosis(group, nan_policy='omit'),\n",
    "        'Q1': group.quantile(0.25),  # First quartile (25th percentile)\n",
    "        'Q3': group.quantile(0.75)   # Third quartile (75th percentile)\n",
    "    })\n",
    "\n",
    "# Apply the descriptive statistics function to each industry in ff_17\n",
    "descriptive_stats_by_ff_17 = results_df_monthly.groupby('ff_17')['ireccd_transformed'].apply(get_descriptive_stats).unstack()\n",
    "\n",
    "# Calculate the overall descriptive statistics across all industries\n",
    "overall_stats = get_descriptive_stats(results_df_monthly['ireccd_transformed'])\n",
    "overall_stats_df = pd.DataFrame(overall_stats).T\n",
    "overall_stats_df.index = ['Overall']\n",
    "\n",
    "# Append the overall statistics at the bottom of the industry-specific statistics\n",
    "final_stats_df = pd.concat([descriptive_stats_by_ff_17, overall_stats_df])\n",
    "\n",
    "# Save the final dataframe with descriptive statistics to a CSV file\n",
    "final_stats_df.to_csv(\"Industry_Descriptive_with_Quartiles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80972be0-6a73-42c1-8db0-8f3636b742dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "      <th>skewness</th>\n",
       "      <th>kurtosis</th>\n",
       "      <th>Q1</th>\n",
       "      <th>Q3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Cars</th>\n",
       "      <td>3.620951</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.958353</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>52447.0</td>\n",
       "      <td>-0.143646</td>\n",
       "      <td>-0.350782</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chems</th>\n",
       "      <td>3.685024</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.960687</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>183871.0</td>\n",
       "      <td>-0.293608</td>\n",
       "      <td>-0.188068</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Clths</th>\n",
       "      <td>3.701181</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.933321</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>31427.0</td>\n",
       "      <td>-0.169168</td>\n",
       "      <td>-0.366749</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cnstr</th>\n",
       "      <td>3.676580</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.951167</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>87119.0</td>\n",
       "      <td>-0.192823</td>\n",
       "      <td>-0.353264</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cnsum</th>\n",
       "      <td>3.740731</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.936590</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11949.0</td>\n",
       "      <td>-0.202912</td>\n",
       "      <td>-0.485330</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           mean  median       std  min  max     count  skewness  kurtosis  \\\n",
       "Cars   3.620951     4.0  0.958353  1.0  5.0   52447.0 -0.143646 -0.350782   \n",
       "Chems  3.685024     4.0  0.960687  1.0  5.0  183871.0 -0.293608 -0.188068   \n",
       "Clths  3.701181     4.0  0.933321  1.0  5.0   31427.0 -0.169168 -0.366749   \n",
       "Cnstr  3.676580     4.0  0.951167  1.0  5.0   87119.0 -0.192823 -0.353264   \n",
       "Cnsum  3.740731     4.0  0.936590  1.0  5.0   11949.0 -0.202912 -0.485330   \n",
       "\n",
       "        Q1   Q3  \n",
       "Cars   3.0  4.0  \n",
       "Chems  3.0  4.0  \n",
       "Clths  3.0  4.0  \n",
       "Cnstr  3.0  4.0  \n",
       "Cnsum  3.0  5.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_stats_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15398b7c-36cb-48f0-ae3a-13b9d15c458f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, recode 'ireccd' so that higher values imply a more favorable recommendation\n",
    "results_df_monthly['ireccd'] = 6 - results_df_monthly['ireccd']\n",
    "\n",
    "# After recoding, proceed with the calculation of the average 'ireccd' as described earlier\n",
    "monthly_rec = results_df_monthly.groupby(['cusip', 'year', 'month']).agg(\n",
    "    avg_ireccd=('ireccd', 'mean'),\n",
    "    sic=('sic', 'first'),\n",
    "    ff_5=('ff_5', 'first'),\n",
    "    ff_10=('ff_10', 'first'),\n",
    "    ff_17=('ff_17', 'first'),\n",
    "    ff_48=('ff_48', 'first')\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "928d4f3e-2b56-4efe-822e-b0e1ab629495",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sort crsp_data by 'cusip' and 'dlycaldt'\n",
    "crsp_data = crsp_data.sort_values(by=['cusip', 'dlycaldt'])\n",
    "\n",
    "# Create a 'year_month' column to group by year and month\n",
    "crsp_data['year_month'] = crsp_data['dlycaldt'].dt.to_period('M')\n",
    "\n",
    "# Calculate the average daily capitalization for each month\n",
    "crsp_data['monthly_avg_cap'] = crsp_data.groupby(['cusip', 'year_month'])['dlycap'].transform('mean')\n",
    "\n",
    "# Shift the monthly average capitalization by one period (one month) within each 'cusip'\n",
    "crsp_data['prior_month_avg_cap'] = crsp_data.groupby('cusip')['monthly_avg_cap'].shift()\n",
    "\n",
    "# Step 1: Calculate daily returns (percentage change in price)\n",
    "crsp_data['daily_return'] = crsp_data.groupby('cusip')['dlyprc'].pct_change(fill_method=None)\n",
    "\n",
    "# Step 2: Calculate absolute daily returns\n",
    "crsp_data['abs_daily_return'] = np.abs(crsp_data['daily_return'])\n",
    "\n",
    "# Step 3: Ensure that 'dlycap' (dollar volume) is non-zero and non-missing\n",
    "crsp_data['dlycap'] = crsp_data['dlycap'].replace(0, np.nan)\n",
    "\n",
    "# Step 4: Calculate the ILLIQ measure for each day as |return| / volume\n",
    "crsp_data['illiq_daily'] = crsp_data['abs_daily_return'] / crsp_data['dlycap']\n",
    "\n",
    "# Step 5: Aggregate ILLIQ to the monthly level\n",
    "# Group by 'cusip' and 'year_month' to calculate the monthly ILLIQ as the mean of the daily values\n",
    "illiq_df = crsp_data.groupby(['cusip', 'year_month']).agg(ILLIQ=('illiq_daily', 'mean')).reset_index()\n",
    "\n",
    "# Step 6: Merge the calculated ILLIQ values back into the main dataset\n",
    "crsp_data = crsp_data.merge(illiq_df, on=['cusip', 'year_month'], how='left')\n",
    "\n",
    "# Step 7: Aggregate data to get the beginning and ending prices, prior month's average capitalization, and standard deviation\n",
    "crsp_monthly = crsp_data.groupby(['cusip', 'year', 'month']).agg(\n",
    "    beginning_price=('dlyprc', 'first'),\n",
    "    ending_price=('dlyprc', 'last'),\n",
    "    cusip9=('cusip9', 'first'),\n",
    "    prior_month_avg_cap=('prior_month_avg_cap', 'first'),\n",
    "    IVOL=('dlyprc', 'std'),\n",
    "    ILLIQ=('ILLIQ', 'first')\n",
    ").reset_index()\n",
    "\n",
    "# Step 8: Calculate the monthly return\n",
    "crsp_monthly['month_return'] = (crsp_monthly['ending_price'] - crsp_monthly['beginning_price']) / crsp_monthly['beginning_price'] * 100\n",
    "\n",
    "# Winsorize the 'month_return' at the 1% and 99% levels\n",
    "crsp_monthly['month_return_winsorized'] = mstats.winsorize(crsp_monthly['month_return'], limits=[0.01, 0.01])\n",
    "\n",
    "# Shift the monthly return by one period (one month) within each 'cusip'\n",
    "crsp_monthly['Rt_1'] = crsp_monthly.groupby('cusip')['month_return_winsorized'].shift(1)\n",
    "\n",
    "# Calculate Rt−12,t−2 (Cumulative return from t−12 to t−2)\n",
    "def cumulative_return(group, start_shift, end_shift):\n",
    "    return group.shift(end_shift).rolling(window=(start_shift - end_shift)).sum()\n",
    "\n",
    "crsp_monthly['Rt_12_t_2'] = crsp_monthly.groupby('cusip')['month_return_winsorized'].transform(\n",
    "    lambda x: cumulative_return(x, 12, 2))\n",
    "\n",
    "# Calculate Rt−60,t−13 (Cumulative return from t−60 to t−13)\n",
    "crsp_monthly['Rt_60_t_13'] = crsp_monthly.groupby('cusip')['month_return_winsorized'].transform(\n",
    "    lambda x: cumulative_return(x, 60, 13))\n",
    "\n",
    "# Shift the monthly illiquidity measure to the prior month by one period (one month) within each 'cusip'\n",
    "crsp_monthly['ILLIQ_prior'] = crsp_monthly.groupby('cusip')['ILLIQ'].shift()\n",
    "\n",
    "# Shift the monthly volatility measure to the prior month by one period (one month) within each 'cusip'\n",
    "crsp_monthly['IVOL_prior'] = crsp_monthly.groupby('cusip')['IVOL'].shift()\n",
    "\n",
    "# Define the function to assign periods based on the year\n",
    "def assign_period(year):\n",
    "    if 1992 <= year <= 1999:\n",
    "        return 1\n",
    "    elif 2000 <= year <= 2009:\n",
    "        return 2\n",
    "    elif 2010 <= year <= 2019:\n",
    "        return 3\n",
    "    elif 2020 <= year <= 2024:\n",
    "        return 4\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Apply the function to create the 'period' column\n",
    "crsp_monthly['period'] = crsp_monthly['year'].apply(assign_period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6cf6c853-f888-4cef-ba1a-5a9aa4cf49ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge monthly_rec with crsp_monthly based on 'cusip', 'year', and 'month'\n",
    "rec_return = pd.merge(monthly_rec, crsp_monthly[['cusip', 'cusip9', 'year', 'month', 'month_return', 'month_return_winsorized', 'IVOL', 'IVOL_prior', 'prior_month_avg_cap', 'ILLIQ', 'ILLIQ_prior', 'Rt_1', 'Rt_12_t_2', 'Rt_60_t_13']], \n",
    "                    on=['cusip', 'year', 'month'], \n",
    "                    how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8757775e-5ca6-499b-8c99-85d3d52d4392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "ff_momentum_path = f\"{input_directory}/FF_Momentum_Monthly.csv\"\n",
    "ff_str_path = f\"{input_directory}/FF_STR_Monthly.csv\"\n",
    "ff3_path = f\"{input_directory}/FF3_Monthly.csv\"\n",
    "ff5_path = f\"{input_directory}/FF5_Monthly.csv\"\n",
    "q_factor_path = f\"{input_directory}/q_factor.csv\"\n",
    "\n",
    "# Function to split Year-Month into separate year and month columns\n",
    "def add_year_month(df, col_name='Year-Month'):\n",
    "    df['year'] = df[col_name].astype(str).str[:4].astype(int)\n",
    "    df['month'] = df[col_name].astype(str).str[4:].astype(int)\n",
    "    return df\n",
    "\n",
    "# Load the datasets\n",
    "ff_momentum = pd.read_csv(ff_momentum_path)\n",
    "ff_str = pd.read_csv(ff_str_path)\n",
    "ff3 = pd.read_csv(ff3_path)\n",
    "ff5 = pd.read_csv(ff5_path)\n",
    "q_factor = pd.read_csv(q_factor_path)\n",
    "\n",
    "# Add year and month columns to each file\n",
    "ff_momentum = add_year_month(ff_momentum)\n",
    "ff_str = add_year_month(ff_str)\n",
    "ff3 = add_year_month(ff3)\n",
    "ff5 = add_year_month(ff5)\n",
    "\n",
    "# Ensure rec_return dataframe has year and month columns\n",
    "if 'year' not in rec_return.columns or 'month' not in rec_return.columns:\n",
    "    rec_return['year'] = rec_return['date'].astype(str).str[:4].astype(int)\n",
    "    rec_return['month'] = rec_return['date'].astype(str).str[5:7].astype(int)\n",
    "\n",
    "# Merge the four datasets into rec_return using lowercase year and month\n",
    "rec_return = pd.merge(rec_return, ff_momentum, on=['year', 'month'], how='left')\n",
    "rec_return = pd.merge(rec_return, ff_str, on=['year', 'month'], how='left')\n",
    "\n",
    "# Merge FF3_Monthly with suffix for overlapping columns\n",
    "rec_return = pd.merge(rec_return, ff3, on=['year', 'month'], how='left', suffixes=('', '_ff3'))\n",
    "\n",
    "# Merge FF5_Monthly with suffix for overlapping columns\n",
    "rec_return = pd.merge(rec_return, ff5, on=['year', 'month'], how='left', suffixes=('', '_ff5'))\n",
    "\n",
    "# Merge q_factor_monthly with suffix for overlapping columns\n",
    "rec_return = pd.merge(rec_return, q_factor, on=['year', 'month'], how='left', suffixes=('', '_q'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b28b0524-7593-48b9-8fa9-09cb562278b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_355139/1370092072.py:4: InvalidColumnName: \n",
      "Not all pandas column names were valid Stata variable names.\n",
      "The following replacements have been made:\n",
      "\n",
      "    Year-Month_x   ->   Year_Month_x\n",
      "    Mom      ->   Mom___\n",
      "    Year-Month_y   ->   Year_Month_y\n",
      "    Year-Month   ->   Year_Month\n",
      "    Mkt-RF   ->   Mkt_RF\n",
      "    Year-Month_ff5   ->   Year_Month_ff5\n",
      "    Mkt-RF_ff5   ->   Mkt_RF_ff5\n",
      "\n",
      "If this is not what you expect, please make sure you have Stata-compliant\n",
      "column names in your DataFrame (strings only, max 32 characters, only\n",
      "alphanumerics and underscores, no Stata reserved words)\n",
      "\n",
      "  rec_return.to_stata(\"rec_return.dta\")\n"
     ]
    }
   ],
   "source": [
    "# Ensure the winsorized columns are converted to numeric data type\n",
    "rec_return['month_return_winsorized'] = pd.to_numeric(rec_return['month_return_winsorized'], errors='coerce')\n",
    "\n",
    "rec_return.to_stata(\"rec_return.dta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4fb72ab6-1431-41fe-b6a3-3f03af9372b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data from WRDS\n",
    "yearly_data = db.raw_sql('''\n",
    "    SELECT gvkey, cusip, datadate, fyear, at, mkvalt, sstk, dv, dvt, ceq \n",
    "    FROM comp.funda\n",
    "    WHERE fyear >= 1990 AND indfmt = 'INDL' AND datafmt = 'STD' AND popsrc = 'D' AND consol = 'C' AND datadate <= '2024-06-01'\n",
    "''', date_cols=['datadate'])\n",
    "\n",
    "# Download quarterly data - No advertising for quarterly\n",
    "quarterly_data = db.raw_sql('''\n",
    "    SELECT gvkey, cusip, datadate, fqtr, fyearq, atq, revtq, oancfy, cogsq, invtq, xrdq, xsgaq, apalchy, mkvaltq, ceqq, actq, cheq, lctq, dlcq, txpq, dpq, dlttq, ppentq, mibq, pstkq, oiadpq, ltq, ajexq, rectq, epspxq\n",
    "    FROM comp.fundq\n",
    "    WHERE datadate >= '1990-01-01' AND indfmt = 'INDL' AND datafmt = 'STD' AND popsrc = 'D' AND consol = 'C' AND datadate <= '2024-06-01'\n",
    "''', date_cols=['datadate'])\n",
    "\n",
    "#create filling date to prevent look-ahead bias\n",
    "#see https://www.investor.gov/introduction-investing/investing-basics/glossary/form-10-k\n",
    "def calculate_filing_date_year(row):\n",
    "    market_cap = row['mkvalt']  # In millions    \n",
    "    if market_cap >= 700:\n",
    "        return row['datadate'] + pd.DateOffset(days=60)  # Large Accelerated Filer\n",
    "    elif 75 <= market_cap < 700:\n",
    "        return row['datadate'] + pd.DateOffset(days=75)  # Accelerated Filer\n",
    "    else:\n",
    "        return row['datadate'] + pd.DateOffset(days=90)  # Non-accelerated Filer\n",
    "    \n",
    "# Apply the function to create the 'filling_date' column\n",
    "yearly_data['filling_date'] = yearly_data.apply(calculate_filing_date_year, axis=1)\n",
    "\n",
    "def calculate_filing_date_qtr(row):\n",
    "    market_cap = row['mkvaltq']  # In millions\n",
    "    fqtr = row['fqtr']\n",
    "    \n",
    "    # Determine if it's a 10-K or 10-Q based on fqtr\n",
    "    if fqtr == 4:  # 10-K\n",
    "        if market_cap >= 700:\n",
    "            return row['datadate'] + pd.DateOffset(days=60)  # Large Accelerated Filer\n",
    "        elif 75 <= market_cap < 700:\n",
    "            return row['datadate'] + pd.DateOffset(days=75)  # Accelerated Filer\n",
    "        else:\n",
    "            return row['datadate'] + pd.DateOffset(days=90)  # Non-accelerated Filer\n",
    "    else:  # 10-Q (fqtr 1, 2, or 3)\n",
    "        if market_cap >= 75:\n",
    "            return row['datadate'] + pd.DateOffset(days=40)  # Large and Accelerated Filers\n",
    "        else:\n",
    "            return row['datadate'] + pd.DateOffset(days=45)  # Non-accelerated Filer\n",
    "\n",
    "# Apply the function to create the 'filling_date' column\n",
    "quarterly_data['filling_date'] = quarterly_data.apply(calculate_filing_date_qtr, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1cdacdbd-dd8b-493c-93f6-e106a82ca684",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uchenna_onuoha001_umb_edu/.local/lib/python3.11/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/uchenna_onuoha001_umb_edu/.local/lib/python3.11/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/uchenna_onuoha001_umb_edu/.local/lib/python3.11/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/uchenna_onuoha001_umb_edu/.local/lib/python3.11/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/uchenna_onuoha001_umb_edu/.local/lib/python3.11/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/uchenna_onuoha001_umb_edu/.local/lib/python3.11/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/tmp/ipykernel_355139/3656171539.py:71: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  quarterly_data = quarterly_data.groupby('gvkey').apply(calc_sue)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:153\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:182\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '/'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 89\u001b[0m\n\u001b[1;32m     86\u001b[0m quarterly_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSalesGrowth\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (quarterly_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrevtq\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m-\u001b[39m quarterly_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrevtq_lag\u001b[39m\u001b[38;5;124m'\u001b[39m] ) \u001b[38;5;241m/\u001b[39mquarterly_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrevtq_lag\u001b[39m\u001b[38;5;124m'\u001b[39m] \n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Return on Equity\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m quarterly_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mROE\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m quarterly_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moiadpq\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m/\u001b[39m (\u001b[43mquarterly_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m  \u001b[38;5;241m-\u001b[39m quarterly_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mltq_lag\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# Asset turnover\u001b[39;00m\n\u001b[1;32m     92\u001b[0m quarterly_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAssetTurnover\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m quarterly_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrevtq\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m/\u001b[39m quarterly_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124matq_lag\u001b[39m\u001b[38;5;124m'\u001b[39m] \n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/core/frame.py:4090\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4089\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4090\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4092\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/core/indexes/base.py:3809\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3805\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3806\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3807\u001b[0m     ):\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3809\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3810\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3811\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3812\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: '/'"
     ]
    }
   ],
   "source": [
    "# Reset the index and drop the current index to avoid inserting it as a column\n",
    "yearly_data = yearly_data.reset_index(drop=True)\n",
    "quarterly_data = quarterly_data.reset_index(drop=True)\n",
    "\n",
    "# Calculate financial metrics for yearly data\n",
    "yearly_data['BM_yearly'] = np.where(yearly_data['mkvalt'] > 0, np.log(yearly_data['ceq'] / yearly_data['mkvalt']), np.nan)\n",
    "yearly_data['Size_yearly'] = np.where(yearly_data['mkvalt'] > 0, np.log(yearly_data['mkvalt']), np.nan)\n",
    "yearly_data['AG'] = (yearly_data['at'] - yearly_data.groupby('gvkey')['at'].shift(1)) / yearly_data.groupby('gvkey')['at'].shift(1)\n",
    "yearly_data['XFIN'] = (yearly_data['sstk'] - yearly_data['dv']) / yearly_data['at']\n",
    "\n",
    "# Calculate financial metrics for quarterly data\n",
    "quarterly_data['BM_qtr'] = np.where(quarterly_data['mkvaltq'] > 0, np.log(quarterly_data['ceqq'] / quarterly_data['mkvaltq']), np.nan)\n",
    "quarterly_data['Size_qtr'] = np.where(quarterly_data['mkvaltq'] > 0, np.log(quarterly_data['mkvaltq']), np.nan)\n",
    "\n",
    "quarterly_data['revtq_lag'] = quarterly_data.groupby('gvkey')['revtq'].shift(1)\n",
    "quarterly_data['atq_lag'] = quarterly_data.groupby('gvkey')['atq'].shift(1)\n",
    "quarterly_data['ltq_lag'] = quarterly_data.groupby('gvkey')['ltq'].shift(1)\n",
    "\n",
    "quarterly_data['GrossProfit'] = (quarterly_data['revtq'] - quarterly_data['cogsq']) / quarterly_data.groupby('gvkey')['atq'].shift(1)\n",
    "\n",
    "# CBOP Calculation\n",
    "quarterly_data['delta_RECTQ'] = quarterly_data.groupby('gvkey')['rectq'].diff()\n",
    "quarterly_data['delta_INVTQ'] = quarterly_data.groupby('gvkey')['invtq'].diff()\n",
    "\n",
    "# CBOP calculation with APALCHY as the level\n",
    "quarterly_data['CBOP'] = ((quarterly_data['revtq'] - quarterly_data['cogsq'] - quarterly_data['xsgaq'] + quarterly_data['xrdq']) - \n",
    "                          (quarterly_data['delta_RECTQ'] + quarterly_data['delta_INVTQ']) - \n",
    "                          quarterly_data['apalchy']) / quarterly_data.groupby('gvkey')['atq'].shift(1)\n",
    "\n",
    "\n",
    "# Accruals Calculation\n",
    "quarterly_data['delta_ACTQ'] = quarterly_data.groupby('gvkey')['actq'].diff()\n",
    "quarterly_data['delta_CHEQ'] = quarterly_data.groupby('gvkey')['cheq'].diff()\n",
    "quarterly_data['delta_LCTQ'] = quarterly_data.groupby('gvkey')['lctq'].diff()\n",
    "quarterly_data['delta_DLCQ'] = quarterly_data.groupby('gvkey')['dlcq'].diff()\n",
    "quarterly_data['delta_TXPQ'] = quarterly_data.groupby('gvkey')['txpq'].diff()\n",
    "\n",
    "quarterly_data['Accruals'] = (quarterly_data['delta_ACTQ'] - quarterly_data['delta_CHEQ'] - \n",
    "                              (quarterly_data['delta_LCTQ'] - quarterly_data['delta_DLCQ'] - quarterly_data['delta_TXPQ']) - \n",
    "                              quarterly_data['dpq']) / quarterly_data.groupby('gvkey')['atq'].shift(1)\n",
    "\n",
    "quarterly_data['WorkingCapital'] = (quarterly_data['actq'] - quarterly_data['lctq']) / quarterly_data['atq']\n",
    "quarterly_data['STDebt'] = quarterly_data['dlcq'] / quarterly_data['atq']\n",
    "quarterly_data['LTDebt'] = quarterly_data['dlttq'] / quarterly_data['atq']\n",
    "\n",
    "quarterly_data['OpLev'] = (quarterly_data['cogsq'] + quarterly_data['xsgaq']) / quarterly_data['atq']\n",
    "quarterly_data['CashHolding'] = quarterly_data['cheq'] / quarterly_data['atq']\n",
    "\n",
    "quarterly_data['ZScore'] = (3.3 * quarterly_data['oiadpq'] + quarterly_data['revtq'] + \n",
    "                            1.4 * quarterly_data['ceqq'] + \n",
    "                            1.2 * (quarterly_data['actq'] - quarterly_data['lctq'])) / quarterly_data['atq']\n",
    "\n",
    "quarterly_data['PPE'] = quarterly_data['ppentq'] / quarterly_data['atq']\n",
    "\n",
    "# Calculate SUE\n",
    "# Calculate the change in split-adjusted earnings per share (EPSPXQ divided by AJEXQ)\n",
    "quarterly_data['eps_change'] = quarterly_data['epspxq'] / quarterly_data['ajexq']\n",
    "\n",
    "# Calculate the earnings surprise (SUE)\n",
    "quarterly_data['eps_lag4'] = quarterly_data.groupby('gvkey')['eps_change'].shift(4)\n",
    "\n",
    "#Apply the rolling standard deviation for SUE calculation within each group\n",
    "def calc_sue(group):\n",
    "    # Calculate the rolling standard deviation of earnings per share\n",
    "    group['eps_std'] = group['eps_change'].rolling(window=8, min_periods=6).std()\n",
    "    # Calculate SUE\n",
    "    group['SUE'] = (group['eps_change'] - group['eps_lag4']) / group['eps_std']\n",
    "    return group\n",
    "\n",
    "# Apply the function to each group (gvkey)\n",
    "quarterly_data = quarterly_data.groupby('gvkey').apply(calc_sue)\n",
    "\n",
    "# Calculate NOA\n",
    "quarterly_data['NOA'] = ((quarterly_data['atq'] - quarterly_data['cheq']) - \n",
    "                         (quarterly_data['atq'] - quarterly_data['dlcq'] - quarterly_data['dlttq'] - \n",
    "                          quarterly_data['mibq'] - quarterly_data['pstkq'] - quarterly_data['ceqq'])) / quarterly_data['atq']\n",
    "\n",
    "# Calculate R&D as a percentage of total assets\n",
    "quarterly_data['R&D'] = quarterly_data['xrdq'] / quarterly_data['atq']\n",
    "\n",
    "# Ensure 'gvkey' is not an index to avoid ambiguity issues, and avoid duplicate columns\n",
    "if 'gvkey' in quarterly_data.index.names:\n",
    "    quarterly_data = quarterly_data.reset_index(drop=True)\n",
    "\n",
    "# Sales growth\n",
    "quarterly_data['SalesGrowth'] = (quarterly_data['revtq'] - quarterly_data['revtq_lag'] ) /quarterly_data['revtq_lag'] \n",
    "\n",
    "# Return on Equity\n",
    "quarterly_data['ROE'] = quarterly_data['oiadpq'] / (quarterly_data['/']  - quarterly_data['ltq_lag'])\n",
    "\n",
    "# Asset turnover\n",
    "quarterly_data['AssetTurnover'] = quarterly_data['revtq'] / quarterly_data['atq_lag'] \n",
    "\n",
    "# Profit margin\n",
    "quarterly_data['ProfitMargin'] = quarterly_data['oiadpq'] / quarterly_data['revtq']\n",
    "\n",
    "# Total leverage\n",
    "quarterly_data['TotalLev'] = quarterly_data['ltq'] / quarterly_data['atq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "55bbe5fd-8837-44be-a917-30888c7e62be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cusip</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>avg_ireccd</th>\n",
       "      <th>sic</th>\n",
       "      <th>ff_5</th>\n",
       "      <th>ff_10</th>\n",
       "      <th>ff_17</th>\n",
       "      <th>ff_48</th>\n",
       "      <th>cusip9</th>\n",
       "      <th>...</th>\n",
       "      <th>RMW</th>\n",
       "      <th>CMA</th>\n",
       "      <th>RF_ff5</th>\n",
       "      <th>R_F</th>\n",
       "      <th>R_MKT</th>\n",
       "      <th>R_ME</th>\n",
       "      <th>R_IA</th>\n",
       "      <th>R_ROE</th>\n",
       "      <th>R_EG</th>\n",
       "      <th>year_month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00003605</td>\n",
       "      <td>2005</td>\n",
       "      <td>12</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.3160</td>\n",
       "      <td>-0.2427</td>\n",
       "      <td>-0.0336</td>\n",
       "      <td>0.8804</td>\n",
       "      <td>0.2287</td>\n",
       "      <td>1.0930</td>\n",
       "      <td>2005-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00003605</td>\n",
       "      <td>2006</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.65</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.3488</td>\n",
       "      <td>3.0346</td>\n",
       "      <td>6.4413</td>\n",
       "      <td>-0.6797</td>\n",
       "      <td>-0.2611</td>\n",
       "      <td>0.8783</td>\n",
       "      <td>2006-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00003605</td>\n",
       "      <td>2006</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>1.91</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.3322</td>\n",
       "      <td>-0.2924</td>\n",
       "      <td>-0.3950</td>\n",
       "      <td>2.4381</td>\n",
       "      <td>-0.9978</td>\n",
       "      <td>-1.0257</td>\n",
       "      <td>2006-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00003605</td>\n",
       "      <td>2006</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.3642</td>\n",
       "      <td>1.4637</td>\n",
       "      <td>3.1221</td>\n",
       "      <td>-1.3435</td>\n",
       "      <td>0.7652</td>\n",
       "      <td>-0.7877</td>\n",
       "      <td>2006-03-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>00030710</td>\n",
       "      <td>2019</td>\n",
       "      <td>11</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6799.0</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "      <td>Finan</td>\n",
       "      <td>Meals</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.63</td>\n",
       "      <td>-1.25</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.1197</td>\n",
       "      <td>3.8673</td>\n",
       "      <td>0.0017</td>\n",
       "      <td>-1.0307</td>\n",
       "      <td>-1.3054</td>\n",
       "      <td>-0.6352</td>\n",
       "      <td>2019-11-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       cusip  year  month  avg_ireccd     sic   ff_5  ff_10  ff_17  ff_48  \\\n",
       "0   00003605  2005     12         2.0     NaN  Other  Other  Other  Other   \n",
       "1   00003605  2006      1         2.0     NaN  Other  Other  Other  Other   \n",
       "2   00003605  2006      2         2.0     NaN  Other  Other  Other  Other   \n",
       "3   00003605  2006      3         2.0     NaN  Other  Other  Other  Other   \n",
       "42  00030710  2019     11         3.0  6799.0  Other  Other  Finan  Meals   \n",
       "\n",
       "   cusip9  ...   RMW   CMA  RF_ff5     R_F   R_MKT    R_ME    R_IA   R_ROE  \\\n",
       "0     NaN  ...  0.22  0.23    0.32  0.3160 -0.2427 -0.0336  0.8804  0.2287   \n",
       "1     NaN  ... -0.65 -0.45    0.35  0.3488  3.0346  6.4413 -0.6797 -0.2611   \n",
       "2     NaN  ... -0.51  1.91    0.34  0.3322 -0.2924 -0.3950  2.4381 -0.9978   \n",
       "3     NaN  ...  0.06 -0.40    0.37  0.3642  1.4637  3.1221 -1.3435  0.7652   \n",
       "42    NaN  ... -1.63 -1.25    0.12  0.1197  3.8673  0.0017 -1.0307 -1.3054   \n",
       "\n",
       "      R_EG  year_month  \n",
       "0   1.0930  2005-12-01  \n",
       "1   0.8783  2006-01-01  \n",
       "2  -1.0257  2006-02-01  \n",
       "3  -0.7877  2006-03-01  \n",
       "42 -0.6352  2019-11-01  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure 'year_month' column exists in rec_return as a combination of 'year' and 'month'\n",
    "rec_return['year_month'] = pd.to_datetime(rec_return['year'].astype(str) + '-' + rec_return['month'].astype(str) + '-01')\n",
    "\n",
    "# Identify duplicates based on 'cusip9' and 'year_month'\n",
    "duplicates_df = rec_return[rec_return.duplicated(subset=['cusip9', 'year_month'], keep=False)]\n",
    "\n",
    "# Display the duplicate rows\n",
    "duplicates_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f69ce2d9-f805-419d-b5dc-251470f2a882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cusip_x</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>avg_ireccd</th>\n",
       "      <th>sic</th>\n",
       "      <th>ff_5</th>\n",
       "      <th>ff_10</th>\n",
       "      <th>ff_17</th>\n",
       "      <th>ff_48</th>\n",
       "      <th>cusip9</th>\n",
       "      <th>...</th>\n",
       "      <th>oancfy</th>\n",
       "      <th>oiadpq</th>\n",
       "      <th>ppentq</th>\n",
       "      <th>pstkq</th>\n",
       "      <th>rectq</th>\n",
       "      <th>revtq</th>\n",
       "      <th>revtq_lag</th>\n",
       "      <th>txpq</th>\n",
       "      <th>xrdq</th>\n",
       "      <th>xsgaq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41135230</td>\n",
       "      <td>1992</td>\n",
       "      <td>12</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6733.0</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "      <td>Finan</td>\n",
       "      <td>Meals</td>\n",
       "      <td>411352305</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41135230</td>\n",
       "      <td>1993</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6733.0</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "      <td>Finan</td>\n",
       "      <td>Meals</td>\n",
       "      <td>411352305</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41135230</td>\n",
       "      <td>1993</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6733.0</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "      <td>Finan</td>\n",
       "      <td>Meals</td>\n",
       "      <td>411352305</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41135230</td>\n",
       "      <td>1993</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6733.0</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "      <td>Finan</td>\n",
       "      <td>Meals</td>\n",
       "      <td>411352305</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>55942410</td>\n",
       "      <td>1993</td>\n",
       "      <td>10</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3612.0</td>\n",
       "      <td>Manuf</td>\n",
       "      <td>Manuf</td>\n",
       "      <td>Machn</td>\n",
       "      <td>ElcEq</td>\n",
       "      <td>559424106</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 116 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    cusip_x  year  month  avg_ireccd     sic   ff_5  ff_10  ff_17  ff_48  \\\n",
       "0  41135230  1992     12         5.0  6733.0  Other  Other  Finan  Meals   \n",
       "1  41135230  1993      1         5.0  6733.0  Other  Other  Finan  Meals   \n",
       "2  41135230  1993      2         5.0  6733.0  Other  Other  Finan  Meals   \n",
       "3  41135230  1993      3         5.0  6733.0  Other  Other  Finan  Meals   \n",
       "4  55942410  1993     10         5.0  3612.0  Manuf  Manuf  Machn  ElcEq   \n",
       "\n",
       "      cusip9  ...  oancfy  oiadpq  ppentq  pstkq  rectq  revtq  revtq_lag  \\\n",
       "0  411352305  ...     NaN     NaN     NaN    NaN    NaN    NaN        NaN   \n",
       "1  411352305  ...     NaN     NaN     NaN    NaN    NaN    NaN        NaN   \n",
       "2  411352305  ...     NaN     NaN     NaN    NaN    NaN    NaN        NaN   \n",
       "3  411352305  ...     NaN     NaN     NaN    NaN    NaN    NaN        NaN   \n",
       "4  559424106  ...     NaN     NaN     NaN    NaN    NaN    NaN        NaN   \n",
       "\n",
       "   txpq  xrdq  xsgaq  \n",
       "0   NaN   NaN    NaN  \n",
       "1   NaN   NaN    NaN  \n",
       "2   NaN   NaN    NaN  \n",
       "3   NaN   NaN    NaN  \n",
       "4   NaN   NaN    NaN  \n",
       "\n",
       "[5 rows x 116 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure 'filling_date' exists and is of datetime type for both yearly and quarterly data\n",
    "yearly_data['filling_date'] = pd.to_datetime(yearly_data['filling_date'])\n",
    "quarterly_data['filling_date'] = pd.to_datetime(quarterly_data['filling_date'])\n",
    "\n",
    "# Convert 'year' and 'month' columns in rec_return to datetime for proper comparison\n",
    "rec_return['year_month'] = pd.to_datetime(rec_return['year'].astype(str) + '-' + rec_return['month'].astype(str) + '-01')\n",
    "\n",
    "# Sort data by cusip and filling_date for proper asof merge behavior\n",
    "yearly_data = yearly_data.sort_values(by=['cusip', 'filling_date'])\n",
    "quarterly_data = quarterly_data.sort_values(by=['cusip', 'filling_date'])\n",
    "rec_return = rec_return.sort_values(by=['cusip9', 'year_month'])\n",
    "\n",
    "# Ensure both 'year_month' and 'filling_date' columns are sorted for asof merge\n",
    "rec_return = rec_return.sort_values('year_month')\n",
    "yearly_data = yearly_data.sort_values('filling_date')\n",
    "quarterly_data = quarterly_data.sort_values('filling_date')\n",
    "\n",
    "# Perform asof merge with yearly_data based on 'cusip9' from rec_return and 'cusip' from yearly_data\n",
    "merged_df = pd.merge_asof(\n",
    "    rec_return,\n",
    "    yearly_data[['cusip', 'filling_date'] + yearly_data.columns.difference(['cusip', 'filling_date']).tolist()],\n",
    "    left_on='year_month',        # Key for time comparison from rec_return\n",
    "    right_on='filling_date',     # Key for time comparison from yearly_data\n",
    "    left_by='cusip9',            # 'cusip9' from rec_return\n",
    "    right_by='cusip',            # 'cusip' from yearly_data\n",
    "    direction='backward'         # Ensure we get the last available data before the 'year_month'\n",
    ")\n",
    "\n",
    "# Perform asof merge with quarterly_data based on 'cusip9' from rec_return and 'cusip' from quarterly_data\n",
    "merged_df = pd.merge_asof(\n",
    "    merged_df,\n",
    "    quarterly_data[['cusip', 'filling_date'] + quarterly_data.columns.difference(['cusip', 'filling_date']).tolist()],\n",
    "    left_on='year_month',        # Key for time comparison from rec_return\n",
    "    right_on='filling_date',     # Key for time comparison from quarterly_data\n",
    "    left_by='cusip9',            # 'cusip9' from rec_return\n",
    "    right_by='cusip',            # 'cusip' from quarterly_data\n",
    "    direction='backward'         # Ensure we get the last available data before the 'year_month'\n",
    ")\n",
    "\n",
    "# Drop unnecessary 'filling_date' columns after merge\n",
    "merged_df = merged_df.drop(columns=['filling_date_x', 'filling_date_y'], errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ad06dcf6-f4ae-4bfe-b68e-ea6f748d90f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_355139/2940942919.py:5: InvalidColumnName: \n",
      "Not all pandas column names were valid Stata variable names.\n",
      "The following replacements have been made:\n",
      "\n",
      "    Year-Month_x   ->   Year_Month_x\n",
      "    Mom      ->   Mom___\n",
      "    Year-Month_y   ->   Year_Month_y\n",
      "    Year-Month   ->   Year_Month\n",
      "    Mkt-RF   ->   Mkt_RF\n",
      "    Year-Month_ff5   ->   Year_Month_ff5\n",
      "    Mkt-RF_ff5   ->   Mkt_RF_ff5\n",
      "    R&D   ->   R_D\n",
      "\n",
      "If this is not what you expect, please make sure you have Stata-compliant\n",
      "column names in your DataFrame (strings only, max 32 characters, only\n",
      "alphanumerics and underscores, no Stata reserved words)\n",
      "\n",
      "  merged_df.to_stata(\"rec_return_fama.dta\")\n"
     ]
    }
   ],
   "source": [
    "# Replace infinite values with NaN\n",
    "merged_df = merged_df.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Now save to Stata file\n",
    "merged_df.to_stata(\"rec_return_fama.dta\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
